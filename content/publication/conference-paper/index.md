---
title: 'Guided Online Distillation: Promoting Safe Reinforcement Learning by Offline Demonstration'

authors:
  - admin
  - Jinning Li
  - Banghua Zhu
  - Jiantao Jiao
  - Masayoshi Tomizuka
  - Chen Tang
  - Wei Zhan

author_notes:
  - 'Co-first Author'
  - 'First Author'
  - 'Contributor'
  - 'Contributor'
  - 'Senior Author'
  - 'Contributor'
  - 'Corresponding Author'

date: '2024-01-15T00:00:00Z'
doi: '10.1109/ICRA.2024.7447-7454'

publishDate: '2024-01-15T00:00:00Z'

publication_types: ['paper-conference']

publication: In *2024 IEEE International Conference on Robotics and Automation (ICRA)*
publication_short: In *ICRA 2024*

abstract: Safe Reinforcement Learning (RL) aims to find policies that achieve high rewards while satisfying cost constraints. Existing approaches often struggle with overly conservative exploration, particularly in safety-critical domains like autonomous driving. We propose Guided Online Distillation (GOLD), an innovative offline-to-online safe RL framework that distills large-capacity offline policies into lightweight, computationally efficient policy networks through guided online safe RL training.

summary: A novel framework for promoting safe reinforcement learning by distilling offline expert demonstrations into lightweight, safe policy networks.

tags:
  - Safe Reinforcement Learning
  - Autonomous Driving
  - Policy Distillation
  - Machine Learning

featured: true

url_pdf: 'https://arxiv.org/pdf/2309.09408.pdf'
# url_code: 'https://github.com//guided-online-distillation'
url_project: 'https://sites.google.com/view/guided-online-distillation'

image:
  caption: 'GOLD Framework for Safe RL'
  focal_point: 'Center'
  preview_only: false

projects:
  - safe-reinforcement-learning
  - autonomous-systems

slides: ""
---

## Research Significance

The Guided Online Distillation (GOLD) framework addresses critical challenges in safe reinforcement learning by:

- Mitigating conservative exploration in safety-critical scenarios
- Distilling large-capacity offline policies into lightweight networks
- Demonstrating improved performance in autonomous driving tasks

## Key Contributions

- Proposed an offline-to-online safe RL framework
- Developed a method to extract lightweight policies from expert demonstrations
- Achieved approximately 15% improvement in success rates for real-world traffic-driving tasks
